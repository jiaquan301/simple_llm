#!/usr/bin/env python3
"""
é›¶åŸºç¡€LLMæ•™ç¨‹ - äº¤äº’å¼æ¼”ç¤ºä»£ç 
è¿™ä¸ªæ–‡ä»¶åŒ…å«äº†æ•™ç¨‹ä¸­æ‰€æœ‰çš„ä»£ç ï¼Œå¹¶æä¾›äº†äº¤äº’å¼çš„æ¼”ç¤ºåŠŸèƒ½
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import random

# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
torch.manual_seed(42)
random.seed(42)

class SimpleTokenizer:
    """
    ç®€å•çš„å­—ç¬¦çº§åˆ†è¯å™¨
    
    è¿™ä¸ªåˆ†è¯å™¨æŠŠæ–‡æœ¬è½¬æ¢æˆæ•°å­—ï¼Œè®©è®¡ç®—æœºèƒ½å¤Ÿå¤„ç†
    å°±åƒç»™æ¯ä¸ªå­—ç¬¦åˆ†é…ä¸€ä¸ªèº«ä»½è¯å·ç 
    """
    def __init__(self, text):
        print("ğŸ”¤ åˆå§‹åŒ–åˆ†è¯å™¨...")
        # è·å–æ‰€æœ‰å”¯ä¸€å­—ç¬¦å¹¶æ’åºï¼Œæ„å»ºè¯æ±‡è¡¨
        self.chars = sorted(list(set(text)))
        self.vocab_size = len(self.chars)
        
        # å­—ç¬¦åˆ°ç´¢å¼•çš„æ˜ å°„ï¼ˆå­—ç¬¦ â†’ æ•°å­—ï¼‰
        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}
        # ç´¢å¼•åˆ°å­—ç¬¦çš„æ˜ å°„ï¼ˆæ•°å­— â†’ å­—ç¬¦ï¼‰
        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}
        
        print(f"   è¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
        print(f"   åŒ…å«å­—ç¬¦: {self.chars[:10]}..." if len(self.chars) > 10 else f"   åŒ…å«å­—ç¬¦: {self.chars}")
    
    def encode(self, text):
        """å°†æ–‡æœ¬ç¼–ç ä¸ºtokenç´¢å¼•åˆ—è¡¨"""
        return [self.char_to_idx[ch] for ch in text]
    
    def decode(self, indices):
        """å°†tokenç´¢å¼•åˆ—è¡¨è§£ç ä¸ºæ–‡æœ¬"""
        return ''.join([self.idx_to_char[i] for i in indices])
    
    def demo_encoding(self, text):
        """æ¼”ç¤ºç¼–ç è¿‡ç¨‹"""
        print(f"\nğŸ“ ç¼–ç æ¼”ç¤º:")
        print(f"   åŸå§‹æ–‡æœ¬: '{text}'")
        encoded = self.encode(text)
        print(f"   ç¼–ç ç»“æœ: {encoded}")
        decoded = self.decode(encoded)
        print(f"   è§£ç éªŒè¯: '{decoded}'")
        return encoded

class MultiHeadAttention(nn.Module):
    """
    å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ - Transformerçš„æ ¸å¿ƒç»„ä»¶
    
    å°±åƒäººç±»é˜…è¯»æ—¶çœ¼ç›ä¼šåœ¨æ–‡æœ¬ä¸­æ¸¸èµ°ï¼Œå¯»æ‰¾ç›¸å…³ä¿¡æ¯
    å¤šå¤´æ³¨æ„åŠ›è®©æ¨¡å‹èƒ½å¤ŸåŒæ—¶ä»å¤šä¸ªè§’åº¦ç†è§£æ¯ä¸ªè¯
    """
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        
        print(f"ğŸ§  åˆå§‹åŒ–å¤šå¤´æ³¨æ„åŠ›:")
        print(f"   æ¨¡å‹ç»´åº¦: {d_model}")
        print(f"   æ³¨æ„åŠ›å¤´æ•°: {n_heads}")
        print(f"   æ¯ä¸ªå¤´çš„ç»´åº¦: {self.head_dim}")
        
        # çº¿æ€§å˜æ¢å±‚ï¼šå°†è¾“å…¥æŠ•å½±ä¸ºQueryã€Keyã€Value
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.out_linear = nn.Linear(d_model, d_model)
        
    def forward(self, x):
        batch_size, seq_len, d_model = x.shape
        
        # ç”ŸæˆQueryã€Keyã€Value
        Q = self.q_linear(x)  # æŸ¥è¯¢ï¼šæˆ‘æƒ³æ‰¾ä»€ä¹ˆä¿¡æ¯ï¼Ÿ
        K = self.k_linear(x)  # é”®ï¼šæ¯ä¸ªè¯èƒ½æä¾›ä»€ä¹ˆä¿¡æ¯ï¼Ÿ
        V = self.v_linear(x)  # å€¼ï¼šæ¯ä¸ªè¯çš„å…·ä½“å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ
        
        # é‡å¡‘ä¸ºå¤šå¤´å½¢å¼
        Q = Q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼ˆç›¸ä¼¼åº¦ï¼‰
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # åº”ç”¨å› æœæ©ç ï¼ˆç¡®ä¿åªèƒ½çœ‹åˆ°ä¹‹å‰çš„tokenï¼‰
        # è¿™å¾ˆé‡è¦ï¼šæ¨¡å‹ä¸èƒ½"å·çœ‹"æœªæ¥çš„è¯
        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
        scores.masked_fill_(mask, float('-inf'))
        
        # åº”ç”¨softmaxè·å¾—æ³¨æ„åŠ›æƒé‡
        attention_weights = F.softmax(scores, dim=-1)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡åˆ°Value
        attention_output = torch.matmul(attention_weights, V)
        
        # é‡å¡‘å¹¶é€šè¿‡è¾“å‡ºçº¿æ€§å±‚
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, d_model)
        
        return self.out_linear(attention_output)

class TransformerBlock(nn.Module):
    """
    Transformerå— - åŒ…å«æ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç½‘ç»œ
    
    è¿™æ˜¯Transformerçš„åŸºæœ¬æ„å»ºå•å…ƒï¼Œå°±åƒæ­ç§¯æœ¨çš„åŸºæœ¬å—
    æ¯ä¸ªå—éƒ½è®©æ¨¡å‹å¯¹è¾“å…¥æœ‰æ›´æ·±çš„ç†è§£
    """
    def __init__(self, d_model, n_heads, d_ff):
        super().__init__()
        print(f"ğŸ§± åˆå§‹åŒ–Transformerå—:")
        print(f"   æ¨¡å‹ç»´åº¦: {d_model}")
        print(f"   å‰é¦ˆç½‘ç»œç»´åº¦: {d_ff}")
        
        self.attention = MultiHeadAttention(d_model, n_heads)
        self.norm1 = nn.LayerNorm(d_model)  # å±‚å½’ä¸€åŒ–ï¼šä¿æŒæ•°å€¼ç¨³å®š
        self.norm2 = nn.LayerNorm(d_model)
        
        # å‰é¦ˆç½‘ç»œï¼šå…ˆæ‰©å±•ç»´åº¦ï¼ˆå‘æ•£æ€ç»´ï¼‰ï¼Œå†å‹ç¼©ï¼ˆæ”¶æ•›ç»“è®ºï¼‰
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),  # æ¿€æ´»å‡½æ•°ï¼šå¼•å…¥éçº¿æ€§
            nn.Linear(d_ff, d_model)
        )
        
    def forward(self, x):
        # æ³¨æ„åŠ›æœºåˆ¶ + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        # æ®‹å·®è¿æ¥ï¼šæ–°çŸ¥è¯† + åŸæœ‰çŸ¥è¯†
        attn_output = self.attention(x)
        x = self.norm1(x + attn_output)
        
        # å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        ff_output = self.feed_forward(x)
        x = self.norm2(x + ff_output)
        
        return x

class SimpleLLM(nn.Module):
    """
    ç®€åŒ–ç‰ˆå¤§è¯­è¨€æ¨¡å‹
    
    è¿™æ˜¯æˆ‘ä»¬çš„å®Œæ•´æ¨¡å‹ï¼ŒåŒ…å«äº†ç°ä»£LLMçš„æ‰€æœ‰æ ¸å¿ƒç»„ä»¶
    è™½ç„¶å¾ˆå°ï¼Œä½†åŸç†å’ŒGPTæ˜¯ä¸€æ ·çš„
    """
    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=2, max_seq_len=64):
        super().__init__()
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.max_seq_len = max_seq_len
        
        print(f"\nğŸ¤– åˆå§‹åŒ–SimpleLLM:")
        print(f"   è¯æ±‡è¡¨å¤§å°: {vocab_size}")
        print(f"   æ¨¡å‹ç»´åº¦: {d_model}")
        print(f"   æ³¨æ„åŠ›å¤´æ•°: {n_heads}")
        print(f"   Transformerå±‚æ•°: {n_layers}")
        print(f"   æœ€å¤§åºåˆ—é•¿åº¦: {max_seq_len}")
        
        # TokenåµŒå…¥å±‚ï¼šå°†tokenç´¢å¼•è½¬æ¢ä¸ºå‘é‡
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        
        # ä½ç½®åµŒå…¥å±‚ï¼šä¸ºæ¯ä¸ªä½ç½®æ·»åŠ ä½ç½®ä¿¡æ¯
        self.position_embedding = nn.Embedding(max_seq_len, d_model)
        
        # Transformerå—å †å 
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_model * 4) 
            for _ in range(n_layers)
        ])
        
        # è¾“å‡ºå±‚ï¼šå°†éšè—çŠ¶æ€æ˜ å°„åˆ°è¯æ±‡è¡¨å¤§å°
        self.output_projection = nn.Linear(d_model, vocab_size)
        
        # è®¡ç®—å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in self.parameters())
        print(f"   æ€»å‚æ•°æ•°é‡: {total_params:,}")
        
    def forward(self, x):
        batch_size, seq_len = x.shape
        
        # ç”Ÿæˆä½ç½®ç´¢å¼•
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)
        
        # TokenåµŒå…¥ + ä½ç½®åµŒå…¥
        token_emb = self.token_embedding(x)
        pos_emb = self.position_embedding(positions)
        x = token_emb + pos_emb
        
        # é€šè¿‡Transformerå—
        for transformer_block in self.transformer_blocks:
            x = transformer_block(x)
        
        # è¾“å‡ºæŠ•å½±åˆ°è¯æ±‡è¡¨
        logits = self.output_projection(x)
        
        return logits
    
    def generate(self, tokenizer, prompt, max_new_tokens=50, temperature=1.0, verbose=False):
        """
        ç”Ÿæˆæ–‡æœ¬
        
        å‚æ•°:
        - prompt: è¾“å…¥æç¤º
        - max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        - temperature: æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶éšæœºæ€§ï¼‰
        - verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¿‡ç¨‹
        """
        self.eval()
        
        if verbose:
            print(f"\nğŸ¯ å¼€å§‹ç”Ÿæˆæ–‡æœ¬:")
            print(f"   è¾“å…¥æç¤º: '{prompt}'")
            print(f"   æœ€å¤§ç”Ÿæˆé•¿åº¦: {max_new_tokens}")
            print(f"   æ¸©åº¦å‚æ•°: {temperature}")
        
        # ç¼–ç è¾“å…¥æç¤º
        input_ids = tokenizer.encode(prompt)
        generated_ids = input_ids.copy()
        
        if verbose:
            print(f"   ç¼–ç åçš„è¾“å…¥: {input_ids}")
            print(f"\nğŸ“ ç”Ÿæˆè¿‡ç¨‹:")
        
        with torch.no_grad():
            for i in range(max_new_tokens):
                # ç¡®ä¿è¾“å…¥é•¿åº¦ä¸è¶…è¿‡æœ€å¤§åºåˆ—é•¿åº¦
                if len(generated_ids) >= self.max_seq_len:
                    input_tensor = torch.tensor([generated_ids[-self.max_seq_len:]])
                else:
                    input_tensor = torch.tensor([generated_ids])
                
                # å‰å‘ä¼ æ’­
                logits = self.forward(input_tensor)
                
                # è·å–æœ€åä¸€ä¸ªä½ç½®çš„logits
                next_token_logits = logits[0, -1, :] / temperature
                
                # åº”ç”¨softmaxå¹¶é‡‡æ ·
                probs = F.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, 1).item()
                
                generated_ids.append(next_token)
                
                if verbose and i < 10:  # åªæ˜¾ç¤ºå‰10æ­¥
                    next_char = tokenizer.decode([next_token])
                    print(f"   æ­¥éª¤ {i+1}: ç”Ÿæˆ '{next_char}' (token {next_token})")
        
        result = tokenizer.decode(generated_ids)
        if verbose:
            print(f"\nâœ… ç”Ÿæˆå®Œæˆ!")
            print(f"   æœ€ç»ˆç»“æœ: '{result}'")
        
        return result

def demonstrate_attention():
    """æ¼”ç¤ºæ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œåŸç†"""
    print("\n" + "="*60)
    print("ğŸ” æ³¨æ„åŠ›æœºåˆ¶æ¼”ç¤º")
    print("="*60)
    
    print("\nğŸ’¡ æ³¨æ„åŠ›æœºåˆ¶å°±åƒäººç±»é˜…è¯»æ—¶çš„çœ¼çƒè¿åŠ¨:")
    print("   å½“æˆ‘ä»¬è¯»åˆ°'ä»–'è¿™ä¸ªä»£è¯æ—¶ï¼Œçœ¼ç›ä¼šå›å¤´å¯»æ‰¾'ä»–'æŒ‡çš„æ˜¯è°")
    print("   æ³¨æ„åŠ›æœºåˆ¶è®©è®¡ç®—æœºä¹Ÿèƒ½åšåˆ°è¿™ä¸€ç‚¹")
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ä¾‹å­
    d_model = 8
    n_heads = 2
    attention = MultiHeadAttention(d_model, n_heads)
    
    # æ¨¡æ‹Ÿè¾“å…¥ï¼š3ä¸ªè¯ï¼Œæ¯ä¸ªè¯ç”¨8ç»´å‘é‡è¡¨ç¤º
    x = torch.randn(1, 3, d_model)
    
    print(f"\nğŸ“Š è¾“å…¥å½¢çŠ¶: {x.shape}")
    print("   (æ‰¹æ¬¡å¤§å°=1, åºåˆ—é•¿åº¦=3, æ¨¡å‹ç»´åº¦=8)")
    
    # å‰å‘ä¼ æ’­
    output = attention(x)
    
    print(f"ğŸ“Š è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print("   æ³¨æ„åŠ›æœºåˆ¶æˆåŠŸå¤„ç†äº†è¾“å…¥!")

def demonstrate_tokenizer():
    """æ¼”ç¤ºåˆ†è¯å™¨çš„å·¥ä½œåŸç†"""
    print("\n" + "="*60)
    print("ğŸ”¤ åˆ†è¯å™¨æ¼”ç¤º")
    print("="*60)
    
    sample_text = "äººå·¥æ™ºèƒ½å¾ˆæœ‰è¶£"
    tokenizer = SimpleTokenizer(sample_text)
    
    # æ¼”ç¤ºç¼–ç è¿‡ç¨‹
    tokenizer.demo_encoding("äººå·¥æ™ºèƒ½")
    tokenizer.demo_encoding("å¾ˆæœ‰è¶£")
    
    print(f"\nğŸ” è¯æ±‡è¡¨æ˜ å°„:")
    for char, idx in list(tokenizer.char_to_idx.items())[:5]:
        print(f"   '{char}' â†’ {idx}")

def train_and_demo():
    """è®­ç»ƒæ¨¡å‹å¹¶æ¼”ç¤ºç”Ÿæˆæ•ˆæœ"""
    print("\n" + "="*60)
    print("ğŸ“ æ¨¡å‹è®­ç»ƒä¸ç”Ÿæˆæ¼”ç¤º")
    print("="*60)
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    text = """äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘ã€‚å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åº”ç”¨é¢†åŸŸã€‚"""
    
    print(f"ğŸ“š è®­ç»ƒæ•°æ®é•¿åº¦: {len(text)} å­—ç¬¦")
    
    # åˆå§‹åŒ–åˆ†è¯å™¨å’Œæ¨¡å‹
    tokenizer = SimpleTokenizer(text)
    model = SimpleLLM(vocab_size=tokenizer.vocab_size, d_model=64, n_heads=4, n_layers=2)
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    input_ids = tokenizer.encode(text)
    
    # è®­ç»ƒè®¾ç½®
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    model.train()
    
    print(f"\nğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")
    losses = []
    
    for epoch in range(200):
        # éšæœºé€‰æ‹©ä¸€ä¸ªåºåˆ—ç‰‡æ®µ
        start_idx = random.randint(0, max(0, len(input_ids) - model.max_seq_len - 1))
        end_idx = start_idx + model.max_seq_len
        
        # è¾“å…¥å’Œç›®æ ‡ï¼ˆç›®æ ‡æ˜¯è¾“å…¥å‘å³ç§»åŠ¨ä¸€ä½ï¼‰
        x = torch.tensor([input_ids[start_idx:end_idx]])
        y = torch.tensor([input_ids[start_idx+1:end_idx+1]])
        
        # å‰å‘ä¼ æ’­
        logits = model(x)
        
        # è®¡ç®—æŸå¤±
        loss = F.cross_entropy(logits.view(-1, tokenizer.vocab_size), y.view(-1))
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        losses.append(loss.item())
        
        if epoch % 50 == 0:
            print(f"   Epoch {epoch:3d}, Loss: {loss.item():.4f}")
    
    print(f"âœ… è®­ç»ƒå®Œæˆ! æœ€ç»ˆæŸå¤±: {losses[-1]:.4f}")
    
    # æµ‹è¯•ç”Ÿæˆ
    print(f"\nğŸ¯ æ–‡æœ¬ç”Ÿæˆæµ‹è¯•:")
    test_prompts = ["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ "]
    
    for prompt in test_prompts:
        print(f"\nğŸ“ è¾“å…¥: '{prompt}'")
        generated_text = model.generate(tokenizer, prompt, max_new_tokens=20, temperature=0.8)
        print(f"ğŸ¤– ç”Ÿæˆ: {generated_text}")

def interactive_demo():
    """äº¤äº’å¼æ¼”ç¤º"""
    print("\n" + "="*60)
    print("ğŸ® äº¤äº’å¼æ¼”ç¤º")
    print("="*60)
    
    print("æ¬¢è¿æ¥åˆ°LLMäº¤äº’å¼æ¼”ç¤º!")
    print("ä½ å¯ä»¥é€‰æ‹©ä»¥ä¸‹æ¼”ç¤º:")
    print("1. åˆ†è¯å™¨æ¼”ç¤º")
    print("2. æ³¨æ„åŠ›æœºåˆ¶æ¼”ç¤º") 
    print("3. å®Œæ•´è®­ç»ƒå’Œç”Ÿæˆæ¼”ç¤º")
    print("4. å…¨éƒ¨æ¼”ç¤º")
    
    while True:
        try:
            choice = input("\nè¯·é€‰æ‹© (1-4, æˆ– 'q' é€€å‡º): ").strip()
            
            if choice == 'q':
                print("ğŸ‘‹ å†è§!")
                break
            elif choice == '1':
                demonstrate_tokenizer()
            elif choice == '2':
                demonstrate_attention()
            elif choice == '3':
                train_and_demo()
            elif choice == '4':
                demonstrate_tokenizer()
                demonstrate_attention()
                train_and_demo()
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-4 æˆ– 'q'")
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§!")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    print("ğŸš€ é›¶åŸºç¡€LLMæ•™ç¨‹ - äº¤äº’å¼æ¼”ç¤º")
    print("="*60)
    print("è¿™ä¸ªç¨‹åºå°†å¸¦ä½ ä½“éªŒLLMçš„æ ¸å¿ƒç»„ä»¶å’Œå·¥ä½œåŸç†")
    
    # è¿è¡Œäº¤äº’å¼æ¼”ç¤º
    interactive_demo()

