#!/usr/bin/env python3
"""
é›¶åŸºç¡€LLMæ•™ç¨‹ - éäº¤äº’å¼æ¼”ç¤ºä»£ç 
ç›´æ¥è¿è¡Œæ‰€æœ‰æ¼”ç¤ºï¼Œæ— éœ€ç”¨æˆ·è¾“å…¥
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import random

# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
torch.manual_seed(42)
random.seed(42)

class SimpleTokenizer:
    """ç®€å•çš„å­—ç¬¦çº§åˆ†è¯å™¨"""
    def __init__(self, text):
        self.chars = sorted(list(set(text)))
        self.vocab_size = len(self.chars)
        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}
        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}
    
    def encode(self, text):
        return [self.char_to_idx[ch] for ch in text]
    
    def decode(self, indices):
        return ''.join([self.idx_to_char[i] for i in indices])

class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.out_linear = nn.Linear(d_model, d_model)
        
    def forward(self, x):
        batch_size, seq_len, d_model = x.shape
        
        Q = self.q_linear(x)
        K = self.k_linear(x)
        V = self.v_linear(x)
        
        Q = Q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
        scores.masked_fill_(mask, float('-inf'))
        
        attention_weights = F.softmax(scores, dim=-1)
        attention_output = torch.matmul(attention_weights, V)
        
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, d_model)
        
        return self.out_linear(attention_output)

class TransformerBlock(nn.Module):
    """Transformerå—"""
    def __init__(self, d_model, n_heads, d_ff):
        super().__init__()
        self.attention = MultiHeadAttention(d_model, n_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Linear(d_ff, d_model)
        )
        
    def forward(self, x):
        attn_output = self.attention(x)
        x = self.norm1(x + attn_output)
        
        ff_output = self.feed_forward(x)
        x = self.norm2(x + ff_output)
        
        return x

class SimpleLLM(nn.Module):
    """ç®€åŒ–ç‰ˆå¤§è¯­è¨€æ¨¡å‹"""
    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=2, max_seq_len=64):
        super().__init__()
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.max_seq_len = max_seq_len
        
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(max_seq_len, d_model)
        
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_model * 4) 
            for _ in range(n_layers)
        ])
        
        self.output_projection = nn.Linear(d_model, vocab_size)
        
    def forward(self, x):
        batch_size, seq_len = x.shape
        
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)
        
        token_emb = self.token_embedding(x)
        pos_emb = self.position_embedding(positions)
        x = token_emb + pos_emb
        
        for transformer_block in self.transformer_blocks:
            x = transformer_block(x)
        
        logits = self.output_projection(x)
        
        return logits
    
    def generate(self, tokenizer, prompt, max_new_tokens=50, temperature=1.0):
        self.eval()
        
        input_ids = tokenizer.encode(prompt)
        generated_ids = input_ids.copy()
        
        with torch.no_grad():
            for _ in range(max_new_tokens):
                if len(generated_ids) >= self.max_seq_len:
                    input_tensor = torch.tensor([generated_ids[-self.max_seq_len:]])
                else:
                    input_tensor = torch.tensor([generated_ids])
                
                logits = self.forward(input_tensor)
                next_token_logits = logits[0, -1, :] / temperature
                probs = F.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, 1).item()
                
                generated_ids.append(next_token)
        
        return tokenizer.decode(generated_ids)

def demonstrate_tokenizer():
    """æ¼”ç¤ºåˆ†è¯å™¨"""
    print("\n" + "="*60)
    print("ğŸ”¤ åˆ†è¯å™¨æ¼”ç¤º")
    print("="*60)
    
    sample_text = "äººå·¥æ™ºèƒ½å¾ˆæœ‰è¶£"
    tokenizer = SimpleTokenizer(sample_text)
    
    print(f"ğŸ“š è®­ç»ƒæ–‡æœ¬: '{sample_text}'")
    print(f"ğŸ” è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    print(f"ğŸ“ åŒ…å«å­—ç¬¦: {tokenizer.chars}")
    
    # æ¼”ç¤ºç¼–ç 
    test_text = "äººå·¥æ™ºèƒ½"
    encoded = tokenizer.encode(test_text)
    decoded = tokenizer.decode(encoded)
    
    print(f"\nğŸ“ ç¼–ç æ¼”ç¤º:")
    print(f"   åŸå§‹æ–‡æœ¬: '{test_text}'")
    print(f"   ç¼–ç ç»“æœ: {encoded}")
    print(f"   è§£ç éªŒè¯: '{decoded}'")

def demonstrate_model_training():
    """æ¼”ç¤ºæ¨¡å‹è®­ç»ƒå’Œç”Ÿæˆ"""
    print("\n" + "="*60)
    print("ğŸ“ æ¨¡å‹è®­ç»ƒä¸ç”Ÿæˆæ¼”ç¤º")
    print("="*60)
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    text = """äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘ã€‚å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åº”ç”¨é¢†åŸŸã€‚"""
    
    print(f"ğŸ“š è®­ç»ƒæ•°æ®é•¿åº¦: {len(text)} å­—ç¬¦")
    
    # åˆå§‹åŒ–åˆ†è¯å™¨å’Œæ¨¡å‹
    tokenizer = SimpleTokenizer(text)
    model = SimpleLLM(vocab_size=tokenizer.vocab_size, d_model=64, n_heads=4, n_layers=2)
    
    print(f"ğŸ¤– æ¨¡å‹ä¿¡æ¯:")
    print(f"   è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    print(f"   æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    input_ids = tokenizer.encode(text)
    
    # è®­ç»ƒè®¾ç½®
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    model.train()
    
    print(f"\nğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")
    
    for epoch in range(100):
        # éšæœºé€‰æ‹©ä¸€ä¸ªåºåˆ—ç‰‡æ®µ
        start_idx = random.randint(0, max(0, len(input_ids) - model.max_seq_len - 1))
        end_idx = start_idx + model.max_seq_len
        
        # è¾“å…¥å’Œç›®æ ‡
        x = torch.tensor([input_ids[start_idx:end_idx]])
        y = torch.tensor([input_ids[start_idx+1:end_idx+1]])
        
        # å‰å‘ä¼ æ’­
        logits = model(x)
        
        # è®¡ç®—æŸå¤±
        loss = F.cross_entropy(logits.view(-1, tokenizer.vocab_size), y.view(-1))
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if epoch % 25 == 0:
            print(f"   Epoch {epoch:3d}, Loss: {loss.item():.4f}")
    
    print(f"âœ… è®­ç»ƒå®Œæˆ!")
    
    # æµ‹è¯•ç”Ÿæˆ
    print(f"\nğŸ¯ æ–‡æœ¬ç”Ÿæˆæµ‹è¯•:")
    test_prompts = ["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ "]
    
    for prompt in test_prompts:
        print(f"\nğŸ“ è¾“å…¥: '{prompt}'")
        generated_text = model.generate(tokenizer, prompt, max_new_tokens=15, temperature=0.8)
        print(f"ğŸ¤– ç”Ÿæˆ: {generated_text}")

def demonstrate_attention_concept():
    """æ¼”ç¤ºæ³¨æ„åŠ›æœºåˆ¶æ¦‚å¿µ"""
    print("\n" + "="*60)
    print("ğŸ” æ³¨æ„åŠ›æœºåˆ¶æ¦‚å¿µæ¼”ç¤º")
    print("="*60)
    
    print("ğŸ’¡ æ³¨æ„åŠ›æœºåˆ¶çš„ç”Ÿæ´»ä¾‹å­:")
    print("   æƒ³è±¡ä½ åœ¨å˜ˆæ‚çš„é¤å…é‡Œå’Œæœ‹å‹èŠå¤©")
    print("   ä½ çš„å¤§è„‘ä¼šè‡ªåŠ¨è¿‡æ»¤æ‰å‘¨å›´çš„å™ªéŸ³")
    print("   ä¸“æ³¨äºæœ‹å‹çš„å£°éŸ³")
    print("   è¿™å°±æ˜¯æ³¨æ„åŠ›çš„ä½œç”¨!")
    
    print("\nğŸ§  åœ¨è¯­è¨€ç†è§£ä¸­:")
    print("   å¥å­: 'å°æ˜æŠŠä¹¦æ”¾åœ¨æ¡Œå­ä¸Šï¼Œç„¶åä»–å»äº†å›¾ä¹¦é¦†'")
    print("   å½“è¯»åˆ°'ä»–'æ—¶ï¼Œæ³¨æ„åŠ›ä¼šå›åˆ°'å°æ˜'")
    print("   å› ä¸ºæˆ‘ä»¬çŸ¥é“'ä»–'æŒ‡çš„æ˜¯å°æ˜")
    
    print("\nğŸ”§ è®¡ç®—æœºå¦‚ä½•å®ç°:")
    print("   1. Query (æŸ¥è¯¢): æˆ‘æƒ³äº†è§£ä»€ä¹ˆï¼Ÿ")
    print("   2. Key (é”®): æ¯ä¸ªè¯èƒ½æä¾›ä»€ä¹ˆä¿¡æ¯ï¼Ÿ")
    print("   3. Value (å€¼): æ¯ä¸ªè¯çš„å…·ä½“å†…å®¹")
    print("   4. è®¡ç®—ç›¸å…³æ€§åˆ†æ•°")
    print("   5. æ ¹æ®åˆ†æ•°åŠ æƒç»„åˆä¿¡æ¯")

def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
    print("ğŸš€ é›¶åŸºç¡€LLMæ•™ç¨‹ - å®Œæ•´æ¼”ç¤º")
    print("="*60)
    print("è¿™ä¸ªç¨‹åºå°†å±•ç¤ºLLMçš„æ ¸å¿ƒç»„ä»¶å’Œå·¥ä½œåŸç†")
    
    # è¿è¡Œæ‰€æœ‰æ¼”ç¤º
    demonstrate_tokenizer()
    demonstrate_attention_concept()
    demonstrate_model_training()
    
    print("\n" + "="*60)
    print("ğŸ‰ æ¼”ç¤ºå®Œæˆ!")
    print("="*60)
    print("é€šè¿‡è¿™äº›æ¼”ç¤ºï¼Œä½ å·²ç»äº†è§£äº†:")
    print("âœ… åˆ†è¯å™¨å¦‚ä½•å°†æ–‡å­—è½¬æ¢ä¸ºæ•°å­—")
    print("âœ… æ³¨æ„åŠ›æœºåˆ¶çš„åŸºæœ¬æ¦‚å¿µ")
    print("âœ… å®Œæ•´çš„LLMè®­ç»ƒå’Œç”Ÿæˆè¿‡ç¨‹")
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("   1. å°è¯•ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œè§‚å¯Ÿæ•ˆæœå˜åŒ–")
    print("   2. ä½¿ç”¨æ›´å¤šçš„è®­ç»ƒæ•°æ®")
    print("   3. å­¦ä¹ æ›´é«˜çº§çš„LLMæŠ€æœ¯")
    print("   4. é˜…è¯»ç›¸å…³çš„ç ”ç©¶è®ºæ–‡")

if __name__ == "__main__":
    main()

